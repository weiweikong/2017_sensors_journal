
%  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

% You need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================
\documentclass[journal,article,submit,moreauthors,pdftex,10pt,a4paper]{mdpi} 
%
%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% actuators, admsci, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, atmosphere, atoms, axioms, batteries, bdcc, behavsci, beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci, buildings, carbon, cancers, catalysts, cells, challenges, chemengineering, chemosensors, children, chromatography, climate, coatings, computation, computers, condensedmatter, cosmetics, cryptography, crystals, data, dentistry, designs, diagnostics, diseases, diversity, econometrics, economies, education, electronics, energies, entropy, environments, epigenomes, fermentation, fibers, fishes, fluids, foods, forests, fractalfract, futureinternet, galaxies, games, gastrointestdisord, gels, genealogy, genes, geosciences, geriatrics, healthcare, horticulturae, humanities, hydrology, informatics, information, infrastructures, inorganics, insects, instruments, ijerph, ijfs, ijms, ijgi, ijtpp, inventions, jcdd, jcm, jdb, jfb, jfmk, jimaging, jof, jintelligence, jlpea, jmse, jpm, jrfm, jsan, land, languages, laws, life, literature, logistics, lubricants, machines, magnetochemistry, marinedrugs, materials, mathematics, mca, mti, medsci, medicines, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, molbank, molecules, mps, nanomaterials, ncrna, neonatalscreening, nutrients, ohbm, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, plants, polymers, processes, proteomes, publications, quaternary, recycling, religions, remotesensing, resources, risks, robotics, safety, scipharm, sensors, separations, sexes, sinusitis, socsci, societies, soils, sports, standards, sustainability, symmetry, systems, technologies, toxics, toxins, tropicalmed, universe, urbansci, vaccines, vetsci, viruses, vision, water
%---------
% article
%---------
% The default type of manuscript is article, but can be replaced by: 
% addendum, article, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, correction, conferenceproceedings, conferencereport, expressionofconcern, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimage, letter, newbookreceived, opinion, obituary, projectreport, reply, reprint, retraction, review, perspective, preprints, shortnote, supfile, technicalnote, viewpoint
% supfile = supplementary materials
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figure are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother 
\articlenumber{x}
\doinum{10.3390/------}
\pubvolume{xx}
\pubyear{2017}
\copyrightyear{2017}
\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

\usepackage{amsmath}
\usepackage{titlesec}

\usepackage{graphicx}
\usepackage{longtable,lscape}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{epstopdf} % by Kong
\usepackage{subfigure} % by Kong
\usepackage{amsmath} 
\usepackage{url}
\usepackage{amssymb}
\usepackage[linesnumbered,ruled,algo2e]{algorithm2e} % for algorithms block by Weiwei

%=================================================================
% Full title of the paper (Capitalized)
\Title{Localization Framework for Real-Time UAV Autonomous Landing: An On-ground Deployed Visual Approach}

% If this is an expanded version of a conference paper, please cite it here: enter the full citation of your conference paper, and add $^\dagger$ in the end of the title of this article.
%\conference{Title}

% Authors, for the paper (add full first names)
%\Author{Weiwei Kong $^{1,2,\dagger,\ddagger}$, Tianjiang Hu $^{1,}$*, Daibing Zhang$^{1}$ and Lincheng Shen $^{1}$}
\Author{Weiwei Kong $^{1,2}$, Tianjiang Hu $^{1,}$*, Daibing Zhang$^{1}$, Lincheng Shen $^{1}$ and Jianwei Zhang $^{3}$}

% Authors, for metadata in PDF
\AuthorNames{Weiwei Kong, Tianjiang Hu, Daibing Zhang, Lincheng Shen and Jianwei Zhang}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad College of Mechatronics and Automation, National University of Defense Technology, Changsha, 410073, P. R China; kongww.nudt@gmail.com(W.K.);swimsword@163.com(D.Z);lcshen@nudt.edu.cn(L.S.); zhang@informatik.uni-hamburg.de(J.Z)\\
$^{2}$ \quad Naval Academy of Armament, Beijing, 100161, P.R China\\
$^{3}$ \quad Institute of Technical Aspects of Multimodal systems(TAMS), Department of Computer Science, University of Hamburg, 22527, Germany} 
% Contact information of the corresponding author
\corres{Correspondence: t.j.hu@nudt.edu.cn}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

% Simple summary
%\simplesumm{}

% Abstract (Do not use inserted blank lines, i.e. \\) 
\abstract{	
One of the greatest challenges for Unmanned Aircraft Vehicles(UAVs) applications is the safe landing. While UAV can land safely with the help of the global navigation satellite system(GNSS), the capability of autonomous landing is still limited in GNSS-impaired environments. In this paper, we propose a real-time vision-based localization framework that could track and locate the aircraft precisely by triangular geometry during the final approach. This large baseline system consisits of two separate perception modules on the ground, which are equpipped with optical sensors driving by a pan/tilt unit (PTU). To detect and track the landing target accurately at long distance, we implement the Discriminative Scale Space Tracking(DSST) method in series with the Bounding Box Prediction algorithm. Both simulation and practical experiments prove that this novel visual system caters for a fixed-wing UAV landing assignment.}

% Keywords
\keyword{UAV; Stereo Vision; Localization}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:

%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For Conference Proceedings Papers: add the conference title here
%\conferencetitle{}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only for the journal Gels: Please place the Experimental Section after the Conclusions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}
%The template details the sections that can be used in a manuscript. Note that the order of article sections may differ from the requirements of the journal (e.g. for the positioning of the Materials and Methods section). Please check the instructions for authors page of the journal to verify the correct order. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX related questions please contact Janine Daum at latex-support@mdpi.com.

\section{Introduction}
Over the past few decades, the application of unmanned aircraft has increased enormously in both civil and military scenarios. Although aerial robots have successfully been implemented in several applications, there are still new research directions related to them. Floreano \cite{floreano2015science} and Kumar et al. \cite{kushleyev2013towards} outlined the opportunities and challenges of this developing field, from the model design to high-level perception capability. All of these issues are concentrating on improving the degree of autonomy, which supports UAVs continue to be used in novel and surprising ways.
% 自主降落能力至关重要
No matter fixed-wing or rotor-way platforms, a standard fully unmanned autonomous system(UAS) involves performing takeoffs, waypoint flight and landings. Among them, landing maneuver is the most delicate and critical phase of UAV flights. Two technical reports \cite{manning2004role} argued that nearly 70\% mishaps of Pioneer UAVs encountered during landing process caused by human factors. So a proper assist system is needed to enhance the reliability of landing task. Generally, two main capabilities of the system are required. The first one is navigation and localization of UAVs, and the second one is generating the appropriate guidance command to guide UAVs for a safe landing. In this paper, we mainly focus on the first issue and try to improve the guidance accuracy and robustness.

For manned aircraft, the traditional landing system uses a radio beam directed upward from the ground \cite{mclean1990automatic, stevens2003aircraft}. By measuring the angular deviation from the beam through onboard equipment, the pilot knows the perpendicular displacement of the aircraft in the vertical channel. For the azimuth information, additional equipment is required. However, due to the size, weight, and power (SWaP) constraints, it is impossible to equip these instruments in UAV. Thanks to the GNSS technology, we have seen lots of successful practical applications of autonomous UAVs in outdoor environments such as transportation, aerial photography and intelligent farming. Unfortunately, in some circumstances, such as urban or low altitude operations, the GNSS receiver antenna is prone to lose line-of-sight with satellites and making GNSS unable to deliver high quality position information \cite{farrell1998gps}. So the autonomous landing in unknown or Global Navigation Satellite System(GNSS)-denied environment is still an open problem. 

Visual based approach is an obvious way to achieve the autonomous landing by estimating flight speed and distance to the landing area, in a moment-to-moment fashion. Generally, two types of visual methods can be considered. The first type is the vision-based onboard system, which has been widely studied. The other is to guidance the aircraft using the ground-based camera system. Once the aircraft is detected by the camera during the landing process, its characteristics, such as type, location, heading and velocity, can be derived by the guidance system. Based on these information, the UAV could align itself carefully towards the landing area and adapt its velocity and acceleration to achieve safely landing. In summary, two key elements of the landing problem are detecting the UAV and its motion calculating the location of the UAV relative to the landing filed. 

Motivated by these mentioned challenges, we propose and develop a novel on-ground deployment for the visual landing system. The essential contributions of this work are following:
\begin{itemize}
	\item Benifiting from the rotation capability of the Pan and Tilt Unit (PTU), we deployed the two PTUs separately on the both sides of the runway to setup an extendable baseline and wide-angle field of view (FOV) vision guidance system.
	\item We illustrate the theoretical and engineering analysis of the localization error and its transferring mechanism in practical situations.
	\item We present and implement a tracking-localization iterative framework supported by the DSST and Bounding Box Prediction algorithms.
\end{itemize}
The developed approach is experimentally validated with fair accuracy and better performance in timeliness as well as practicality against the previous works.

The paper is orginzed as follows: 

\section{Related Works}
%\subsection{On-board Vision Method}
%The problem of accurately landing a UAV using vision-based control has been well developed for autonomous rotorcraft. Visual Servo algorithms have been extensively studied in the robotics filed over the past decade. Considering the model as an eye-in-hind configuration, an practical imaged-based visual servo control frame work has been proposed by Guenard \cite{Guenard2008}. By calculating the image error kinematics, a designed Lyapunov controller shows good performance and robustness of hovering a quadrotor UAV. 

%An alternative to template-based method are optical-flow-aided position measurement systems. This approach was inspired by flying insect navigation strategies \cite{Green2004} and have been developed over the years to use onboard terrestrial and aerial robots. Triggered by the observations of honeybees, an optic flow regulator was introduced. It allows a tethered rotorcraft to deal with non-stationary environments and the rotorcraft was able to land safely near a target which mounted on a moving platform both vertically and horizontally. In another recent work \cite{Vlantis2015}, a discrete-time non-linear model predictive controller (MPC) was employed to land a Parrot AR.Drone on a Pioneer mobile robot with inclined platform. The on-board forward facing camera was used for tracking the moving target and the downwards pointing camera for optical flow. For fixed-wing aircraft, Kim et al.\cite{Kim2013} reported a vision-based net recovery system. The ground station received the image capture by the on-board camera aiming to detect a recovery net. The longitudinal and lateral bearing angle were calculated in keeping with a guidance law based on the net relative position. This technique is not suitable for the whole landing process because the vision operating range is only 50 m.
%%
%For cooperative localization some structured landing mark, such as 2D barcodes, helipad, could provide the relative transform between landing area and camera. Sharp et al. \cite{Sharp2001} solved the landing task of a Yamaha R-50 helicopter by using the designed landing target(white squares) for landing area recognition and estimating the relative position. By calculating the multiple view matrix, their proposed algorithms\cite{Shakernia2002} improve the motion and structure estimation. However, the controller only regulates the x and y position to hover over the landing platform. Saripalli et al. also presented a image-moment-based method for autonomous landing of a helicopter on a H-shape landing pad \cite{Saripalli2003}. However, the precise height estimation benefits from the differential GPS system. In 2012, a worthwhile study \cite{richardsonautomated2013} provide a framework for an automated vision-based recovery of an unmanned rotatory craft onto a moving platform. The onboard camera system calculates the displacement and orientation of the UAV respect to the pattern and autonomous maneuvers the craft towards the landing platform. The workable limit for robust tracking was 10 m. For fixed-wing landing, German Aerospace Center (DLR) built and tested the 44-pound drone which successfully spotted a QR code on the roof of an Audi and touched down on the net affixed to the station wagon’s roof, at 43 mph \cite{DLR_Landing}. However, due to the viewing angle of the lens and the size of fiducial marker, the detection distance of the sole of vision control method is limited.
%
%The combination of laser and visual system also has a long history in this field. In 2009, Garratt \cite{garrattvisual2009} considered a relatively low-cost guidance system through visual tracking and LIDAR positioning for autonomous recovery of a Yamaha R-max rotorcraft from ships at sea. This onboard visual system identifies and tracks the single beacon to determine both the distance and the orientation of the deck. Because the wavelength of the beacon is $650\ nm$, the craft can track it in bright sunlight. For the laser system, the laser rangefinder at a wavelength of $780\ nm$ assembling with a spinning mirror was mounted at the bottom of the rotorcraft, which scans a conical pattern on the deck for estimating the position. The error of position estimation is better than $2\ cm$ in practice, but the detection distance is only $100\ m$ due to the size and the power of the selected LED. 
%%
%\subsection{Ground-based Vision Method}
While several techniques have been applied for on-board vision-based control of UAVs, few have shown landing of a fixed-wing guiding by ground-based vision system. Wang \cite{Wang2006} proposed a system using a step motor controlling web camera to track and guide a micro-aircraft. The drawback of this system is the limited baseline leading to a narrow field of view (FOV). To increase the camera FOV, multi-camera systems are considered attractive. This kind of system could solve the common vision problems and track object to compute their 3D locations. At Chiba University \cite{pebrianti2010autonomous}, a ground-based Bumblebee stereo vision system was used to calculate the 3D position of a quadrotor at the altitude of 6 meter. 
%
In addtion, Martinez \cite{Martinez2009a} introduced a trinocular on-ground system, which is composed by three or more cameras for extracting key features of UAV in order to obtain robust 3D position estimation. The range of detection is still not sufficient for fixed-wing UAV. And another drawback of multi-camera system is the calibration process, whose parameters are nontrivial to obtain. Moreover, we also reviewed various vision-based landing approaches performing on different platform \cite{kong2014vision} and Gautam etc. provide another general review of the autonomous landing techniques for UAVs \cite{Gautam2014}.
%
Our group first developed the traditional stereo ground-based system with infrared cameras \cite{kong2013autonomous} while this system has limited detection distance. To enhance the operating capability, we conducted the triangular geometry localization method to the PTU-based system and discussed the error analysis in detail \cite{kong2014ground}. According to the previous work, the localization accuracy largely depends on the aircraft detection precision in the camera image plane. So we implemented Chan-Vese method \cite{tang2016ground} and Saliency-inspired method \cite{ma2016stereo} to detect and track the vehicle more accurately, however, these approaches are not suitable for real-time requirements.

\section{Architechture of Ground-based Stereo System}
In this section, we introduce the theoretical model for ground-to-air visual system. We first recap the traditional stereo vision model which has limited base line restrainting the detection distance. To enlarge the system working boundary, we setup the camera and other sensor module on the two separated PTU and then calculate the target according the image information and rotation angle from PTU. Each vision unit work independently and transfers the results of image processing and PTU status to the navigation computer which calculates the estimated relative position of the UAV. The architecture of the ground stereo vision system is shown in Figure \ref{fig:SystemStructure}.

\begin{figure}[!tb]
	\centering
	\includegraphics[width=0.9\textwidth]{Figs/SystemStructure2.pdf}
	\caption{Architecture of the ground stereo vision system.}
	\label{fig:SystemStructure}
\end{figure}


\subsection{Traditional Stereo Vision Model}
The standard camera model is pin-hole camera Mode. The coordinate of the target $M$ is $(x,y,z)$, and its position on image plane is $(u,v)$. The camera focus is $f$, then the relationship of coordinate between the 3D world and 2D image plane can be calculated by 
\begin{equation}
	\lambda\left[ {\begin{array}{*{20}{c}}
			u \\ 
			v \\ 
			f 
	\end{array}} \right] =\left[ {\begin{array}{*{20}{c}}
			x \\ 
			y \\ 
			z 
	\end{array}} \right]
\end{equation}
where $\lambda$ is the scale factor. 

Although the above model is simple, it could be helpful to estimate the theoretical camera lens according the expected distance and resolution or to measure the target size roughly based on the pixel length on image plane. Let the width and height of the target be $W$ and $H$, the distance between the camera and target be $L$, the target projection on image plane be $w$ and $h$, the relationship between them is  
\begin{align}
	f=\frac{wL}{W} \qquad f=\frac{hL}{H}
\end{align}

We define the coordinates of left and right guidance module as shown in Figure \ref{fig:Fig04_GeneralSystem}. When the optical axes of these two cameras are parallel, we could calculate the target in 3D space by
\begin{equation}
	\left[ {\begin{array}{*{20}{c}}
			x \\ 
			y \\ 
			z 
	\end{array}} \right] =\frac{b}{d} \left[ {\begin{array}{*{20}{c}}
			u_l \\ 
			u_r \\ 
			f 
	\end{array}} \right]
\end{equation}
where $b$ is the baseline and $d=u_l-u_r$ is the pixel disparity. The traditional stereo vision model as shown in Figure \ref{fig:chp03_vision_20_basic_stereo}. Even though some calibration methods could manage the axes unparallel situation, it is still difficult to calculate the system correctly as the baseline is large.
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{Figs/Fig04_GeneralSystem.pdf}
	\caption{The coordinates definition of the separated stereo visual gudiance system.}
	\label{fig:Fig04_GeneralSystem}
\end{figure*}

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/chp03_vision_20_basic_stereo.pdf}	
	\caption{Theoretical Stereo Vision Model}
	\label{fig:chp03_vision_20_basic_stereo}
\end{figure}


\subsection{Separate Stereo Vision System Configuration}
In order to detect the target at long distance, the large base line, more than $5\ m $, is required. Benefiting the camera assembled on PTU separately, we could switch the baseline freely according to the expected detection distance and target size.

In this paper, we assumed that the world coordinate system $(X, Y, Z)$ is located on the origin of the left vision unit, the rotation center of the PTU. For the sake of simplicity, the camera is installed on the PTU in the way that the axes of the camera frame are parallel to those of PTU frame. The origins of these two frames are close. So it can be assumed that the camera frame coincides with the body frame. Figure  \ref{fig:TheoreticalModel} reveals the theoretical model for visual measurement. After installing the right camera system on ${X}$-axis, the left and right optical center can be expressed as ${O_l}$ and ${O_r}$ respectively. Then the base line of the optical system is $O_lO_r$ whose distance is ${D}$. Considering the center of mass of UAV as a point ${M}$, ${O_lM}$ and ${O_rM}$ illustrate the connections between the each optical center and the UAV. In addition, ${\phi_l}$, ${\phi_r}$, ${\psi_l}$, ${\psi_r}$ denote the tilt and pan angle on both side. Therefore, we define $\phi_l= 0$, $\phi_r=0$, ${\psi_l=0}$ and ${\psi_r=0}$, as the PTU is set to the initial state, i.e. the optical axis parallel the runway; the measurement of a counterclockwise direction is positive.

\begin{figure}[!tb]
	\centering
	\includegraphics[height=6cm]{figs/Fig03_Stereo.pdf}	
	\caption{Theoretical Optical Model. The target $M$ is projected at the center of the optical axis.}
	\label{fig:TheoreticalModel}
\end{figure}

Since the point ${M}$ not coincides with the principle point which is the center of image plane, the pixel deviation compensation in the longitudinal and horizontal direction should be considered. As shown in Figure  \ref{fig:Fig02_ImagePlaneOnly}, we calculate pixel deviation compensation on the left side by 

\begin{equation} 
\left \{
\begin{split}
& \psi_{cl} = \arctan \frac{(u-u_0)du}{f} \\
& \phi_{cl} = \arctan \frac{(v-v_0)\cos\psi_{cl}dv}{f} 
\end{split}
\right.
\end{equation}
where optical point is $o(u_o,v_o)$, $du$ and $dv$ are the pixel length of the $u$- and $v$-axis in image plane, and $f$ is the focus. The current PTU rotation angle can be directly obtained through the serial ports during the experiments. Let $\phi_{pl}$ and $\psi_{pl}$ be the left pan and tilt angle separately. Then, the total pan and tilt angle on the left side can be detailed as:
\begin{equation} 
\left \{
\begin{split}
\phi_l &= \phi_{cl} + \phi_{pl} \\ 
\psi_l &= \psi_{cr} + \psi_{pr}
\end{split}
\right.
\end{equation}

\begin{figure*}[!th]
	\centering
	\includegraphics[width=0.6\textwidth]{Figs/chp03_vision_02_image_plane.pdf}
	\caption{The geometry of one PTU with respect to optical center and image plane.}
	\label{fig:Fig02_ImagePlaneOnly}
\end{figure*}

For the other side, we could also calculate the angle in the same way. 

The world coordinates of point ${M}$ is $(x_M, y_M, z_M)\in \textbf{R}^3 $. Point $N$ is the vertical projection of point $M$ on $XOY$ plane, and $NA$ is perpendicular to $X$-axis. If we define $NA = h$, the following guidance parameters can be obtained:
\begin{equation}
\left \{
\begin{aligned}
&x_M = h \tan \psi_l = \frac{D\tan \psi_l}{\tan \psi_l - \tan \psi_r}            \\
&y_M = h = \frac{D}{\tan \psi_l - \tan \psi_r} \\
&z_M = \frac{h\tan \phi_l}{\cos \psi_l} = \frac{D\tan \phi_l}{\cos \psi_l(\tan \psi_l - \tan \psi_r)}
\end{aligned} \right.
\label{eq:M_Positon_Equation}
\end{equation} 


% For use of infraed camera system
Also, errors in the internal and external camera calibration parameters marginally affects some of the estimates - the x-position and z-position, in particular.

\subsection{Error Analysis}
We are now in the position to analysis the error related to the PTU rotation angle. According to (\ref{eq:M_Positon_Equation}, the partial derivative of each equation respect to the pan angle and the tilt angle are denoted in the following way,

\begin{equation}
	\left\{ \,
	\begin{aligned}
		\frac{ \partial x_M}{ \partial \psi_l} = \frac{D \tan \psi_r}{ \cos^2 \psi_l (\tan \psi_l - \tan \psi_r)^2} \\
		\frac{ \partial x_M}{\partial \psi_r} = \frac{D \tan \psi_l}{\cos^2 \psi_r (\tan \psi_l - \tan \psi_r)^2} 
	\end{aligned}
	\right.
\end{equation}

\begin{equation}
	\left\{ \,
	\begin{aligned}
		\frac{\partial y_M}{\partial \psi_l} = \frac{ D}{\cos^2 \psi_l (\tan \psi_l - \tan \psi_r)^2} \\
		\frac{\partial y_M}{\partial \psi_r} = \frac{D}{\cos^2 \psi_r (\tan \psi_l - \tan \psi_r)^2} 
	\end{aligned}
	\right.	
\end{equation}
\begin{equation}
	\left\{ \,
	\begin{aligned}
		&\frac{ \partial z_M}{ \partial \phi_l} = \frac{D}{ \cos \psi_l \cos^2 \phi_l (\tan \psi_l - \tan \psi_r)} \\
		&\frac{\partial z_M}{\partial \psi_l} = \frac{ D \tan \phi_l(\cos \psi_l + \sin \psi_l \tan \psi_r)}{ \cos^2 \psi_r (\tan \psi_l - \tan \psi_r)^2} \\
		&\frac{ \partial z_M}{ \partial \psi_r} = \frac{ D \tan \phi_l}{ \cos \psi_l \cos^2 \psi_r (\tan \psi_l - \tan \psi_r)^2}
	\end{aligned}
	\right.
\end{equation} 

To analyze the influence of the error from the angle, we define the gradient of the world coordinate as
\begin{equation}
	\nabla_{x_M}(\psi_l, \psi_r):=\left( \frac{\partial x_M}{\partial \psi_l}(\psi_l, \psi_r), \frac{\partial x_M}{\partial \psi_r}(\psi_l, \psi_r)  \right)
\end{equation}

\begin{equation}
	\nabla_{y_M}(\psi_l, \psi_r):=\left( \frac{\partial y_M}{\partial \psi_l}(\psi_l, \psi_r), \frac{\partial y_M}{\partial \psi_r}(\psi_l, \psi_r)  \right)
\end{equation}

\begin{equation}
	\nabla_{z_M}(\psi_l, \psi_r):=\left( \frac{\partial z_M}{\partial \psi_l}(\psi_l, \psi_r), \frac{\partial z_M}{\partial \psi_r}(\psi_l, \psi_r)  \right)
\end{equation}


In this case, simulation is needed to evaluate the behavior of our visual system. Figure  \ref{fig:ErrorVectorFieldX}, Figure  \ref{fig:ErrorVectorFieldY}, and Figure  \ref{fig:ErrorVectorFieldZ} are the vector field distribution of $\nabla_{x_M}(\psi_l, \psi_r)$, $\nabla_{y_M}(\psi_l, \psi_r)$, and $\nabla_{z_M}(\psi_l, \psi_r)$, which give us an intuitive result under different types of errors. The length of each vector describes the strength at a specific point; the direction along the vector points to the direction of the fastest error increase. However, only when $y_M \geq 0$ (the aircraft is in front of two cameras), the area $\psi_l - \psi_r > 0$ has the physics meaning. Figure  \ref{fig:ErrorVectorFieldX4} shows that $x_M$ has a significant variation when $\psi_l$ is approximate to $\psi_r$, namely the optical axes are nearly parallel. Further, $y_M$ and $z_M$ have the similar variations. Considering the general working status of the ground-based system, we mainly focus on the second  quadrant of aforementioned vector fields as shown in \ref{fig:ErrorVectorFieldX4}, \ref{fig:ErrorVectorFieldY4} and \ref{fig:ErrorVectorFieldZ4}. In these area, there are slight variation that theoretically demonstrates the feasibility of the system. 

%Since the derivative of $z_M$ also respect to $\alpha$, we present the variation of $\frac{\partial z_M}{\partial \phi_l}$ respect to $\phi_l$ under different $\psi_l$ as shown in Figure  \ref{fig:ErrorAlphaTheta}. The result indicates that we should plan the descending curve of the approaching phase properly in order to keep $\phi_l$ at an adaptive angle, avoiding large-magnitude errors. 
 
\begin{figure*}[!tb]
	\centering
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldX}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_03_glide_3_x_with_theta_l_r.pdf}
	}
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldY}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_04_glide_3_y_with_theta_l_r.pdf}
	}
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldZ}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_05_glide_3_z_with_theta_l_r.pdf}
	}
	\caption{Vector Field Distribution of (a) $\nabla_{x_M}(\psi_l, \psi_r)$,  (b) $\nabla_{y_M}(\psi_l, \psi_r)$, and (c) $\nabla_{z_M}(\psi_l, \psi_r)$}
\end{figure*}

\begin{figure*}[!tb]
	\centering
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldX4}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_06_glide_3_x_with_theta_l_r_2_quadrant.pdf}
	}
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldY4}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_08_glide_3_z_with_theta_l_r_2_quadrant.pdf}
	}
	\subfigure[]
	{
		\label{fig:ErrorVectorFieldZ4}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_06_glide_3_x_with_theta_l_r_2_quadrant.pdf}
	}
	\subfigure[]
	{
		\label{fig:chp03_vision_10_glide_3_gradient_y_2_quadrant}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_10_glide_3_gradient_y_2_quadrant.pdf}
	}
	\subfigure[]
	{
		\label{fig:}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_11_glide_3_gradient_z_2_quadrant.pdf}
	}	
	\subfigure[]
	{
		\label{fig:chp03_vision_09_glide_3_gradient_x_2_quadrant}
		\includegraphics[height=3.5cm]{Figs/chp03_vision_09_glide_3_gradient_x_2_quadrant.pdf}
	} 
	\caption{(a)-(c) Vector Field Distribution of $\nabla_{x_M}(\psi_l, \psi_r)$, $\nabla_{y_M}(\psi_l, \psi_r)$, and $\nabla_{z_M}(\psi_l, \psi_r)$ in Second Quadrant. (d)-(f) Gradient of X, Y, Z with $\theta_l$ and  $\theta_r$ in Second Quadrant}
\end{figure*}

%
\subsection{Practical Analysis}
In theory, $O_lM$ and $O_rM$ should intersect perfectly at one point all the time as shown in Figure  \ref{fig:TheoreticalModel}. Due to the inevitable errors from PTU rotation and tracking algorithms, we estimates the intersecting point by combing the vertical line of two different plane in space.

(1) We set $(x_{ol}, y_{ol}, z_{ol})=(0, 0, 0)$ and $(x_{or}, y_{or}, z_{or})=(D, 0, 0)$ are the optical center of each camera. Assuming that $a_l\neq0$, $b_l\neq0$, $c_l\neq0$ and $a_r\neq0$, $b_r\neq0$, $c_r\neq0$, we obtain the parametric equations of line $O_lM$ and $O_rM$ 
\begin{equation}  
\left \{
\begin{split}
&\frac{x-x_{ol}}{a_l} = \frac{y-y_{ol}}{b_l} = \frac{z-z_{ol}}{c_l} = t_l,\\
&\frac{x-x_{or}}{a_r} = \frac{y-y_{or}}{b_r} = \frac{z-z_{or}}{c_r} = t_r,
\end{split}
\right.
\end{equation}
\begin{equation}  
\left\{ 
\begin{array}{lll} 
a_l = \cos \phi_l \sin \psi_l\\
b_l = \cos \phi_l \cos \psi_l\\
c_l = \sin \phi_l
\end{array} 
\right.
\qquad
\left\{ 
\begin{array}{lll} 
a_r = \cos \phi_r \sin \psi_r\\
b_r = \cos \phi_r \cos \psi_r\\
c_r = \sin \phi_r
\end{array} 
\right.
\end{equation}
where $t_l$, $t_r$ are the parameters for the line $O_lM$ and $O_rM$ separately.
So any point $(x, y, z)$ on each line are usually written parametrically as a function of $t_l$ and $t_r$:
\begin{equation}  
\left\{ 
\begin{array}{lll} 
x_l = a_l t_l + x_{ol} \\
y_l = b_l t_l + y_{ol} \\
z_l = c_l t_l + z_{ol}
\end{array} 
\right.
\qquad
\left\{ 
\begin{array}{lll} 
x_r = a_r l_r + x_{or} \\
y_r = b_r t_r + y_{or} \\
z_r = c_r t_r + z_{or}
\end{array} 
\right.
\end{equation}

(2) In our situation, $O_lM$ and $O_rM$ are skew lines that these two lines to not be parallel and to not intersect in 3D. Generally, the shortest distance between the two skew lines lies along the line which is perpendicular to both of them. By defining the intersection points of the shortest segment line for each line by $(x_{lp}, y_{lp}, z_{lp})$ and $(x_{rp}, y_{rp}, z_{rp})$, we get the parametric equations
\begin{equation}  
\left\{ 
\begin{array}{lll} 
x_{lp} = a_l t_l + x_{ol} \\
y_{lp} = b_l t_l + y_{ol} \\
z_{lp} = c_l t_l + z_{ol}
\end{array} 
\right.
\qquad
\left\{ 
\begin{array}{lll} 
x_{rp} = a_r l_r + x_{or} \\
y_{rp} = b_r t_r + y_{or} \\
z_{rp} = c_r t_r + z_{or}
\end{array} 
\right.
\end{equation}

(3) Knowing the position of the intersection points on each line, the distance is calculated by the square Euclidean norm 
\begin{equation}
J = \|(x_{lp}, y_{lp}, z_{lp}) - (x_{rp}, y_{rp}, z_{rp}) \|_2^2
\end{equation}
According to the distance function, we could derive the position of the target point.
%% Example of a theorem:
\begin{Theorem}
The position of the target point M in the world coordinate is
\begin{flalign}
\begin{bmatrix}
x_M \\ 
y_M \\
z_M
\end{bmatrix}
=w
\begin{bmatrix}
x_{lp} \\ 
y_{lp} \\
z_{lp}
\end{bmatrix}
+(1-w)
\begin{bmatrix}
x_{rp} \\ 
y_{rp} \\
z_{rp}
\end{bmatrix}
, w \in [0,1].
\end{flalign}
where $w$ is weitht, and the other parameters are
\small
\begin{equation}
\left\{ 
\begin{aligned}
x_{lp}=a_lD \frac{ a_l (a_l^2 + b_l^2 + c_l^2) - a_r (a_la_r + b_lb_r + c_lc_r)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2} \\
y_{lp}=b_lD \frac{ a_l (a_l^2 + b_l^2 + c_l^2) - a_r (a_la_r + b_lb_r + c_lc_r)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2} \\
z_{lp}=c_lD \frac{ a_l (a_l^2 + b_l^2 + c_l^2) - a_r (a_la_r + b_lb_r + c_lc_r)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2}
\end{aligned}
\right.
\end{equation}
and 
\begin{equation}
\left\{
\begin{aligned}
&x_{rp}=D \left[ a_r\frac{a_l(a_la_r + b_lb_r + c_lc_r)  - a_r (a_l^2 + b_l^2 + c_l^2)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2}+ 1\right]  \\
&y_{rp}=b_rD\frac{ a_l(a_la_r + b_lb_r + c_lc_r)  - a_r (a_l^2 + b_l^2 + c_l^2)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2} \\
&z_{rp}=c_rD \frac{ a_l(a_la_r + b_lb_r + c_lc_r)  - a_r (a_l^2 + b_l^2 + c_l^2)}{ (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2}
\end{aligned}
\right.
\end{equation}
\normalsize
\end{Theorem}

%% Example of a proof:
\begin{proof}[Proof of Theorem 1]
The expansion of the Eucidean distance is
\begin{equation}  	
J = {\left( {{a_l}{t_l} - {a_r}{t_r} + {x_{ol}} - {x_{or}}} \right)^2} + {\left( {{b_l}{t_l} - {b_r}{t_r} + {y_{ol}} - {y_{or}}} \right)^2} + {\left( {{c_l}{t_l} - {c_r}{t_r} + {z_{ol}} - {z_{or}}} \right)^2}
\end{equation}
Seconldy, $t_l$ and $t_r$ are two parameters of the distance function $J$, and the partial derivative of them are
\begin{equation}  	
\frac{{\partial J}}{{\partial {t_l}}} = 2{a_l}\left( {{a_l}{t_l} - {a_r}{t_r} + {x_{ol}} - {x_{or}}} \right) + 2{b_l}\left( {{b_l}{t_l} - {b_r}{t_r} + {y_{ol}} - {y_{or}}} \right) + 2{c_l}\left( {{c_l}{t_l} - {c_r}{t_r} + {z_{ol}} - {z_{or}}} \right) 
\end{equation}
\begin{equation} 
\frac{{\partial J}}{{\partial {t_r}}} =  - 2{a_r}\left( {{a_l}{t_l} - {a_r}{t_r} + {x_{ol}} - {x_{or}}} \right) - 2{b_r}\left( {{b_l}{t_l} - {b_r}{t_r} + {y_{ol}} - {y_{or}}} \right) - 2{c_r}\left( {{c_l}{t_l} - {c_r}{t_r} + {z_{ol}} - {z_{or}}} \right) 
\end{equation}
Those two skew lines belong to two separated parallel planes. So our target is to find the optimal position by minimizing the distance between the intersection points when
\begin{equation} 
\frac{{\partial J}}{{\partial {t_l}}} = 0
\end{equation}
\begin{equation} 
\frac{{\partial J}}{{\partial {t_r}}} = 0
\end{equation}
Then, the above functions derive the following equation
\begin{flalign}  
&
\begin{bmatrix}
a_l^2 + b_l^2 + c_l^2       & -(a_la_r + b_lb_r + c_lc_r) \\
-(a_la_r + b_lb_r + c_lc_r) & a_l^2 + b_l^2 + c_l^2 \\    
\end{bmatrix}	
\begin{bmatrix}
t_l \\ 
t_r 
\end{bmatrix} \nonumber \\
&=(x_{ol}-x_{or})
\begin{bmatrix}
-a_l \\
a_r 
\end{bmatrix}
+(y_{ol}-y_{or})
\begin{bmatrix}
-b_l \\
b_r 
\end{bmatrix} \nonumber
+(z_{ol}-z_{or})
\begin{bmatrix}
-c_l \\
c_r
\end{bmatrix}.
\end{flalign}
We could define the matrix at left side as:
\begin{flalign} 
\mathbf{H} = 
\begin{bmatrix} 
a_l^2 + b_l^2 + c_l^2      & -(a_la_r + b_lb_r + c_lc_r) \\  -(a_la_r + b_lb_r + c_lc_r) & a_l^2 + b_l^2 + c_l^2 \\ 
\end{bmatrix} 
\end{flalign}
We denote $\det \mathbf{H} $ for the determinant of this matrix. 

When $MO_l$ and $MO_r$ are parallel to each other, so $\det \mathbf{H} = 0$. We only considering that there is an uniqueness vertical line, so $\det \mathbf{H} \neq 0$ and solve the equation
\begin{equation} 
\begin{gathered}
\left[ {\begin{array}{*{20}{c}}
	{{t_l}} \\ 
	{{t_r}} 
	\end{array}} \right] =  - {{\mathbf{H}}^{ - 1}}D\left[ {\begin{array}{*{20}{c}}
	{ - {a_l}} \\ 
	{{a_r}} 
	\end{array}} \right] \\ 
=  - D\left[ {\begin{array}{*{20}{c}}
	{\frac{{a_r^2 + b_r^2 + c_r^2}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}}&{\frac{{{a_l}{a_r} + {b_l}{b_r} + {c_l}{c_r}}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}} \\ 
	{\frac{{{a_l}{a_r} + {b_l}{b_r} + {c_l}{c_r}}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}}&{\frac{{a_l^2 + b_l^2 + c_l^2}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}} 
	\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
	{ - {a_l}} \\ 
	{{a_r}} 
	\end{array}} \right] \\ 
=  - D\left[ {\begin{array}{*{20}{c}}
	{ - {a_l}\frac{{a_r^2 + b_r^2 + c_r^2}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}} + {a_r}\frac{{{a_l}{a_r} + {b_l}{b_r} + {c_l}{c_r}}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}} \\ 
	{ - {a_l}\frac{{{a_l}{a_r} + {b_l}{b_r} + {c_l}{c_r}}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}} + {a_r}\frac{{a_l^2 + b_l^2 + c_l^2}}{{{{\left( {{a_l}{b_r} - {b_l}{a_r}} \right)}^2} + {{\left( {{b_l}{c_r} - {c_l}{b_r}} \right)}^2} + {{\left( {{a_l}{c_r} - {c_l}{a_r}} \right)}^2}}}} 
	\end{array}} \right] \\ 
\end{gathered}
\end{equation}

The parameter $t_l$ and $t_r$ can be expressed as 
\begin{flalign}
\left\{
\begin{aligned}
t_l=D \frac{\displaystyle a_l (a_l^2 + b_l^2 + c_l^2) - a_r (a_la_r + b_lb_r + c_lc_r)}{\displaystyle (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2} \\
t_r=D \frac{\displaystyle a_l(a_la_r + b_lb_r + c_lc_r)  - a_r (a_l^2 + b_l^2 + c_l^2)}{\displaystyle (a_lb_r-b_la_r)^2 + (b_lc_r-c_lb_r)^2 + (a_lc_r-c_la_r)^2}
\end{aligned}
\right.
\end{flalign}
By defining the weight $w$, we could evaluate the position of target point.
\end{proof}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/Fig06_ErrorSurf2000.pdf}
	\caption{Baseline is $15\ m$. Focus is $100\ mm$. The Mersurement Errors in $X$, $Z$ and $Y$ axis with $2000\ m$.}
	\label{fig:Fig06_ErrorSurf2000}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/Fig07_ErrorSurf200.pdf}
	\caption{Baseline is $15\ m$. Focus is $100\ mm$. The Mersurement Errors in $X$, $Z$ and $Y$ axis with $280\ m$.}
	\label{fig:Fig06_ErrorSurf200}
\end{figure*}


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/chp03_vision_15_long_range_error_d20_f100.pdf}
	\caption{Baseline is $20\ m$. Focus is $100\ mm$. The Mersurement Errors in $X$, $Z$ and $Y$ axis with $2000\ m$.}
	\label{fig:chp03_vision_15_long_range_error_d20_f100}
\end{figure*}


The angle between the UAV landing trajectory and the runway area is usually between $3 \degree$ and $7 \degree$. By considering $1\ rad$ normal distributed disturbance (the accuracy of the PTU is $0.006 \degree$), Figure \ref{fig:Fig06_ErrorSurf2000} illustrates measurement errors of $x_M$, $y_M$ and $z_M$ in case of different points $(x,y) \in S$, where $S= \{ (x,y)| -50 \leq x \leq 50, 20 \leq y \leq 1000 \}$. 

Obviously, the errors at a considerable distance are notable, but the incidence of them decline while the aircraft is close to the runway. When the UAV is only $100\ m$ to the landing area, the error of altitude is about $0.02\ m$ that is dependable for the landing task as shown in Figure \ref{fig:Fig06_ErrorSurf200}. Figure \ref{fig:chp03_vision_15_long_range_error_d20_f100} shows that the guidance could be improved at the same distance with a large base line configuration ($20\ m$). 

Also, the errors are much smaller as the UAV lands align the center line of the runway. Table \ref{label:chp03_stereo_1} and Table \ref{label:chp03_stereo_2} illustrate that the error various non-linear and decrease significantly as the target approaches to the touch down point. The smaller the disturbance of the system is, the better the accuracy in each axis will be.

\begin{table}[t]
	\centering
	\caption{Errors List when Disturbance of the System is $1\%$, $f=100\ mm$ and $D=10\ m$}
	\label{label:chp03_stereo_1}
	\begin{tabular}{crrrrrrrrr}
		\hline
		Error (m)/Distance (m)     & 4000    & 3000   & 2000     & 1000  & 500   & 200   & 100   & 50     \\ \hline
		Xerror  & -1.44   & -1.16  & -0.63  & -0.65   & -0.25 & -0.10 & -0.05 & -0.05\\ 
		Yerror  & 1141.92 & 692.31 & 333.44 & 195.65  & 23.82 & 3.92  & 1.00  & 0.25   \\
		Zerror  & 133.42  & 82.62  & 39.24  & 22.70   & 2.43  & 0.28  & 0.03  & -0.02  \\ \hline		
	\end{tabular}
\end{table}

\begin{table}[t]
	\centering
	\caption{Errors List when Disturbance of the System is $5\%$, $f=100\ mm$ and $D=10\ m$}
	\label{label:chp03_stereo_2}
	\begin{tabular}{crrrrrrrrr}
		\hline
		Error (m)/Distance (m)     & 4000    & 3000    & 2000      & 1000  & 500    & 200   & 100   & 50    \\ \hline
		Xerror  & -3.33   & -3.00   & -2.53   & -2.14   & -1.02  & -0.46 & -0.23 & -0.13 \\ 
		Yerror & 2663.28 & 1800.31 & 1000.11 & 642.87  & 100.23 & 18.19 & 4.17  & 1.23  \\
		Zerror & 320.66  & 214.93  & 117.73  & 74.57   & 10.24  & 1.32  & 0.09  & -0.07 \\ \hline		
	\end{tabular}
\end{table}


\subsection{Implementation}
For short-baseline configuration, cameras were setup on one PTU, as shown in Figure  \ref{fig:chp08_18_ground_short_ptus}, and the system should mount on the center line of the runway. However, the short-baseline limit the range for UAV detection. According to our discussion in last section, we could also fix the cameras with separated PTU on the both sides of the runway, as shown in Figure  \ref{fig:SystemOutsideRealPic}, so the landing aircraft can be locked by our system around $1\ km$. Benifiting from the PTU, the searching coverage of the system is much larger than the traditional parallel stereo configuration whose detection field is constrained by the FOV of the camera.

%The glide slope is the ratio of the distance from the last waypoint to the touchdown point. We generally set this ratio less than 10\% to avoid crashing.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\textwidth]{Figs/chp08_18_ground_short_ptus.pdf}	
	\caption{Short-baseline Ground System}
	\label{fig:chp08_18_ground_short_ptus}
\end{figure}


\begin{figure}[!tb]
	\centering
	\subfigure[]
	{
		\includegraphics[height=4cm]{Figs/PTUwithCamera.pdf}
		\label{fig:PTUwithCamera}
	}
	\subfigure[]
	{
		\includegraphics[height=4cm]{Figs/SystemOutsideRealPic.pdf}
		\label{fig:SystemOutsideRealPic}
	}
	\caption{(a) Assembled vision system. (b) The setup of the visual system.}
\end{figure}
%
\subsection{PTU Control}

Proper control of the PTU system can keep the landing target in the field of view of the camera. Figure \ref{fig:Fig02_ImagePlaneOnly} shows the focal frame ($f_x$, $f_y$, $f_z$), constructed about the focal point of the camera, $F$, and the image plane is ($u, $v).  In our application, we assume that the axis of rotation for pan and tilt coincide with the two optical center. The rotation angles are defined as $(\phi, \theta, \psi)$. The angle $\psi$, as shown in Figure  \ref{fig:Fig02_ImagePlaneOnly}, between the target and the $f_y$ axis is calculated by
\begin{equation}
	tan(\phi)=\frac{v-v_0}{l}
\end{equation}
\begin{equation}
	tan(\psi)=\frac{u-u_0}{f}
\end{equation}
where $(u_0, v_0)$ is the principle point in pixel, $(u, v)$ is the current target coordinate in pixel, $f$ is the focus in pixel and $l$ is the direct distance from the optical center to the projected point of the target onto $u$ axis. It can be inferred from the Fig \ref{fig:Fig02_ImagePlaneOnly} that
\begin{align} \label{eq:FOV_TILT}
	\phi &=f(v, v_0, w_v, \alpha_{FOV_{tilt}}) \\
	&=atan(v-v_0, \frac{w_v}{2}cot \frac{\alpha_{FOV_{tilt}}}{2})
\end{align}
\begin{align} \label{eq:FOV_PAN}
	\psi &=f(u, u_0, w_u, \alpha_{FOV_{pan}}) \\
	&=atan(u-u_0, \frac{w_u}{2}cot\frac{\alpha_{FOV_{pan}}}{2})
\end{align}
where $w_u$ and $w_v$ are size of image plane in pixel, $\alpha_{FOV_{pan}}$ and $\alpha_{FOV_{tilt}}$ are the filed of view in radians. There is no rotation around Y axis, so $\theta = 0$. Let the target coordinate be at $(u_1, v_1)$, the desired location be at $(u_2, v_2)$. Using equation (\ref{eq:FOV_TILT} and (\ref{eq:FOV_PAN}, the resulting of pan and tilt compensation to keep the target to the desired location in the image plane is given by
\begin{equation}
	\Delta\phi=f(v_1,v_0,w_v, \alpha_{FOV_{tilt}})-f(v_2,v_0, w_v, \alpha_{FOV_{tilt}})
\end{equation}
\begin{equation}
	\Delta\psi=f(u_1,u_0,w_u, \alpha_{FOV_{pan}})-f(u_2,u_0, w_u, \alpha_{FOV_{pan}})
\end{equation}
Generally, we let the desired location be the principle point in order to keep the target projecting at the center area of the image plane.

\section{Large Base Line Vision System Calibration}
Tradition checkerboard pattern calibration method is not sufficient and convenient to obtain the system parameters for our large base line system. Different from the traditional binocular vision system, the optical axes of each vision unit are not parallel during the operation and there are initial offset between the camera optical center and the rotation axes of PTU. We setup the setting points with the help of the DGPS module and calibrate the system based on the PTU rotation angle, coordiantes and the ground-truth position of the setting points.
\subsection{DGPS-based Calibration Method}
$\mathcal{F}^{path,l}$($\mathbf{i}^{path,l}$,$\mathbf{j}^{path,l}$, $\mathbf{k}^{path,l}$) is the landing path coordinate system whose original point is the touch down point. The goal of the calibration method is to evaluate the rotation and transition matrix between the $\mathcal{F}_{path,l}$ and the local guidance coordinate $\mathcal{O}_c$($\mathbf{i}^{O,c}$,$\mathbf{j}^{O,c}$, $\mathbf{k}^{O,c}$) which coincides with the initial state of the left guidance system. Figure \ref{fig:10_DGPS_Calibration} illustrates the coordinate systems and the arrangement of setting points.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{Figs/10_DGPS_Calibration.pdf}	
	\caption{DGPS-based Calibration Configuration}
	\label{fig:10_DGPS_Calibration}
\end{figure}

%相比于视觉系统的棋盘格标定方法，DGPS标定方法的优势是能够满足远距离标定需求。一般DGPS的设置点距离转台在$100\ m$以上，可以覆盖引导系统的末状态工作区域。而传统棋盘格由于制作尺寸受限，其标定范围一般在$5~10\ m$，该范围并不是转台经常工作的区域，其标定效果一般较差。

The target position during the landing progress in path coordinate denotes as $(X_p, Y_p, Z_p)$ and in guidance coordinate as $(X_c, Y_c, Z_c)$. The transformation can be defined as 
\begin{equation}
\left[ {\begin{array}{*{20}{c}}
	{{X_p}} \\ 
	{{Y_p}} \\ 
	{{Z_p}} 
	\end{array}} \right] = \mathcal{R}_c^p\left[ {\begin{array}{*{20}{c}}
	{{Z_{c}}} \\ 
	{{Y_{c}}} \\ 
	{{Z_{c}}} 
	\end{array}} \right] + T_c^p
\end{equation}
where $\mathcal{R}_c^p$ is a $3\times3$ rotation matrix, following the $\psi-\theta-\phi$ rotation sequence,
%其中$\mathcal{R}_c^p$是$3\times3$旋转矩阵，$T_c^p$是$3\times1$平移矩阵。由于转换关系符合欧拉角定义，按照$\psi-\theta-\phi$的转换顺序，可以得到
\begin{equation}
\mathcal{R}_c^p = \begin{bmatrix}
\cos \theta \cos \psi                             & \cos\theta \sin\psi                               & -\sin\theta         \\
-\cos\phi \sin\psi + \sin\phi \sin\theta \cos\psi & \cos\phi \cos\psi + \sin\phi \sin\theta\sin\psi   & \sin\phi \cos\theta \\
\sin\phi \sin\psi + \cos\phi \sin\theta \cos\psi  & -\sin\phi \cos\psi + \cos\phi \sin\theta \sin\psi & \cos\phi \cos\theta
\end{bmatrix}
\end{equation}
%上述转换矩阵中的欧拉角无法通过解析的方法求得，只能通过多次采集标定点数据最优迭代获得。因此，定义旋转矩阵和平移矩阵的分量为
and $T_c^p$ is a $3\times1$ tranisition matrix,
\begin{equation}
T_c^p=\left[ {\begin{array}{*{20}{c}}
	t_x \\ 
	t_y \\ 
	t_z 
	\end{array}} \right]
\end{equation}
%当通过手动控制转台使得光心对准DGPS天线时，上述6个变量$(\phi\ \theta\ \psi\ t_x\ t_y\ t_z)$与转台角度俯仰角$\phi_l$和方位角$\psi_l$之间的关系为

When we set the setting point in the filed of view, we could manually rotate the PTU till the setting point is at the optical center of the image plane. Obtaining the current rotation angle from PTU, the relationship between the target position in the landing path coordinate and the rotation angle is
\begin{equation}
\left\{ \begin{gathered}
\tan \phi_l = \frac{Y_p}{Z_p} \\
\tan \phi_l = \frac{X_p}{\sqrt{X_p^2+Z_p^2} }\\
\end{gathered}  \right.
\end{equation}
Using the abbreviation form of rotation matrix, 
\begin{equation}
\mathcal{R}_c^p = \begin{bmatrix}
r_{11} & r_{12} & r_{13}\\
r_{21} & r_{22} & r_{23}\\
r_{31} & r_{32} & r_{33}\\
\end{bmatrix}
\end{equation}
we obtain
\begin{equation}
\left\{ \begin{gathered}
\tan \phi_l= \frac{r_{21}X_p + r_{22}Y_p + r_{23}Z_p + t_y}{r_{31}X_p + r_{32}Y_p + r_{33}Z_p + t_z} \\
\tan \psi_l= \frac{r_{11}X_p + r_{12}Y_p + r_{13}Z_p + t_y}{\sqrt{(r_{31}X_p + r_{32}Y_p + r_{33}Z_p + t_y)^2+(r_{21}X_p + r_{22}Y_p + r_{23}Z_p + t_y)^2} }\\
\end{gathered}  \right.
\end{equation}
After mounting the setting point at various position, we calculate the calibration parameters $(\phi\ \theta\ \psi\ t_x\ t_y\ t_z)$ using non-linear least square method.
%二维转台DGPS方法标定的基本步骤如下：
%\begin{compactenum}
%	\item
%	转台系统初始化，并归零。
%	\item
%	放置DGPS点在不同位置（不少于3组，一般为10组），并该点的DGPS数据和转台两个转动角度。
%	\item
%	将采集到的数据通过非线性最小二乘的方法求解得到上述6个标定参数。
%\end{compactenum}
%注意在标定过程中，尽可能保持DGPS横向（$\mathbf{i}^{O,c}$轴向）移动的连续性，使得转台始终向一侧转动。由于转动机械机构的误差，转台出现连续向左和向右的运动，容易将误差放大。

\subsection{DGPS-based Calibration Experiments}
To obtain the precise parameters, we arrange the setting point with DGPS module at different position at the distance around $100\ m$. We fix the DGPS receiver module at a tripod and point the camera to each setting point manually. To reduce the mechanical rotation error, we arrange the setting point from left side to right side. Figure \ref{fig:chp03_vision_18_dgps_calibration_diagram} shows the configuration of setting points in a calibration progress.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{Figs/chp03_vision_18_dgps_calibration_diagram.pdf}	
	\caption{The 20 Calibration Setting Points during the Single PTU Calibration Process}
	\label{fig:chp03_vision_18_dgps_calibration_diagram}
\end{figure}

We set the base line is $10\ m$ and sampled 20 setting point data pairs. Selecting 10 of these sample data to calculate the calibration parameters, the other 10 pairs to evaluate the accuracy. Six calibration parameters are lists in Tabel \ref{label:dgps_calibration}. Figure \ref{fig:chp03_vision_19_pan_tilt_ten_points_error} shows the pan and tilt error. The average error in pan axis is $0.0141 \degree$ and in tilt axis is $-0.0319\degree$.

\begin{table}[htb]
	\centering
	\caption{Parameters for DGPS-based Calibration}
	\label{label:dgps_calibration}
	\begin{tabular}{ccccccc}
		\hline
		Parameters & $\phi (\degree)$ & $\theta (\degree)$ & $\psi (\degree)$ & $t_x (m)$ & $t_y(m)$ & $t_z(m)$ \\ \hline
		Value & 1.58             & -0.01              & 1.57             & -0.41     & 23.31   & -2.21    \\ \hline
	\end{tabular}
\end{table}

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{Figs/chp03_vision_19_pan_tilt_ten_points_error.pdf}	
	\caption{Calibration Error in Pan and Tilt Axis}
	\label{fig:chp03_vision_19_pan_tilt_ten_points_error}
\end{figure}




\section{Tracking and Localization Framework}
%In our previous work \cite{ma2016stereo}\cite{tang2016ground}\cite{hu2016ros}, those target tracking algorithms only consider the current frame. In other words, the tracking algorithms detect the object solely by measurements taken in the current image while TLD method uses patches found on the past trajectory and the detection module could reinitialize the bonding box in case of losing the target. To keep the camera pointing to the target closely, we designed a vision tracking framework based on TLD and bounding box shrinking algorithm. Also, the camera rotation angle in pan and tilt axis, which can be received through the serial port from PTU, can also improve the tracking accuracy. 

Our perivous work \cite{ma2016stereo}\cite{tang2016ground}\cite{hu2016ros} introduced saliency-inspired and Chan-Vese Model method to track the UAV during the landing progress. These algorithms consider merely the current frame infromation, which is mainly the ROI region, to detect and track the target. As the aircraft approaching, its size changes sharply and the wing rotates frequently. Those varieties of the target make the local position calculation, which highly deponds on the target center coordinates in the image, diffcult and error prone. To keep the guidance system pointing to the target tightly and update the tracking model adapti vely, we introduced a vison tracking framework based on DSST method and bounding box prediction. Figure \ref{fig:11_DSST_Method} shows the data flow of the tracking framework.
%
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{Figs/11_DSST_Method.pdf}	
	\caption{PTU-based UAV Tracking Framework}
	\label{fig:11_DSST_Method}
\end{figure*}
\subsection{DSST Method}
The original idea of using correlation filter to sovle the translation situation in target tracking problem is the Minimum Output Sum of Squared Errors(MOOSE)\cite{bolme2010visual} tracker, which works by updating the discriminative correlation filter on a series of sampled image patches. This tracker can estimate the target translation in the next frame, however, it ignores the scale variation. The DSST method\cite{danelljan2014accurate} additionaly calculates a one-dimensional discriminative scale filter to evaluate the target size. Those two discriminative filters are
\begin{itemize}
	\item The translation filter calculates the grey scale of each pixel and a 20-dimension FHOG feature and evalualtes the target position based on those features.
	\item The scale filter rescale the target in last frame in 30 different scales and extracts the 20-dimension FHOG feature in each of those rescaled image patch. The optimal scale factor is estimated based on the FHOG features.
\end{itemize}
\subsubsection{Translation Filter}
In gerneral, each patch can be featured in a $d$-demension vector. For a patch or a single sample $F$, those features in different demensions can be noted as $F^l,\ l \in {1, 2, ..., d}$. A correlation filter $H$, which consists of the these features, is achieved by minimizing the $L^2$ error of the correlation response
\begin{align}
\label{eq:Correlation}
	\underset{{{H}^{*}}}{\mathop{\min }}\,\sum\limits_{l=1}^{d}{|{{H}^{*}}\odot{{F}^{l}}-{{G}^{l}}{{|}^{2}}}+\lambda \sum_{l=1}^{d}|H^{l}|^2
\end{align}
where ${H}^{*}$ is the conjugate of $H$, $\odot$ denotes the circular correlation and $G^l$ is the desired correlation output. The second term in Equation \ref{eq:Correlation} is a regularization where the $\lambda$ is a weight parameter. This optimal result is
\begin{align}
\label{eq:OptimalHFunction}
H^l\text{=}\frac{\sum\limits_{i=1}^{m}{{{F}^{l}}\odot {G^{i}}^{{*}}}}{\sum\limits_{i=1}^{d}{{{F}^{i}}\odot {F^{i}}^{\text{*}}}+\lambda}=\frac{A^{l}_{t}}{B^{l}_{t}+\lambda}
\end{align}
As discussed in MOOSE, Equation \ref{eq:OptimalHFunction} can be solved by calculating the numerator and denominator independently. So the update functions are
\begin{align}
\label{eq:moose_at_update}
&A_t^l=(1-\mu)A_{t-1}^{l}+\mu G_{t}^{*}\odot F_{t}^{l}\\
&B_t^l=(1-\mu)B_{t-1}^{l}+\mu \sum_{l=1}^{d}{F_{t}^{l}}^{*}\odot F_{t}^{l}
\end{align}
After revising the filter, given a new frame, we did inverse discrete Fourier transform(DFT) to obtain the location the target by 
 \begin{align}
 \mathbf{y}_t=\mathcal{F}^{-1}(\frac{\sum_{i=1}^{d}{A^{l}_{t-1}}^*\odot Z^{l}_{t}}{B_{t-1}+\lambda})
 \end{align}
As shown in Figure \ref{fig:chp04_09_translation_map}, the translation filter porgress divided into three steps. The first step is to extract HOG features and gray scale feature and the second step is to calculate the response map in the ROI region. Then searching the maximum correlation score at the last step. The position of the hightest value in response map, as shown in red star, is the predict target position in the image plane.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{Figs/chp04_09_translation_map.pdf}	
	\caption{Translation Filter Progress}
	\label{fig:chp04_09_translation_map}
\end{figure*}

\subsubsection{Scale Filter}
The scale correlation filter has one dimension. To did the correlation between the scale filter and the ROI region, we rescale the ROI region, which size is $P\times R$, in the last frame with $S$ discreted scale factors. Instead of using one scale factor as introduced in the orignal DSST method, we use horizontal and vertical scale factors separately to give the consideration to the situation that the UAV turning and rotate slightly during the landing procedure. Let $a=1.05$ be the horizontal scale factor and $b=0.02$ be the vertical scale factor, the rescale filter sequence is 
\begin{align}
a^nP_{t-1}\times b^nR_{t-1}\ ,n \in \{-\frac{S-1}{2},...,\frac{S-1}{2}\} 
\end{align}
Extracting the HOG features from the 20 rescaled patches, we did the inverse DFT to get the scale factor response map. So the scale filter works similar as the translation filter progress, which shown in Figure \ref{fig:chp04_10_scale_map}. The optimal horizontal and vertical factor are noted as red stars.
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{Figs/chp04_10_scale_map.pdf}	
	\caption{Scale Filter Progress}
	\label{fig:chp04_10_scale_map}
\end{figure*}


%
%
%\subsection{SRC-TLD Algorithm}
%TLD\cite{kalal2012tracking} is an adaptive tracking-by-learning approach, which tracks by learning the appearance of the target - using the online learned classifier to detect the object during the progress. It is consist of three components: 
%\begin{itemize}
%	\item Tracker is based on Lucas-Kanade method, which could track the target frame by frame and generate examples for learning components.
%	
%	\item Detector is to solve the classification problem based on random forest algorithm.
%	
%	\item Learning uses trackers to generate positive and negative samples which is mainly based on P-N Learning mechanisms. Also it update the inner model of the detector.
%\end{itemize}
%All of these three components runs independently during the calculation and the general frame work as shown in Figure  \ref{fig:sci03_tld_framework}.
%
%\begin{figure*}[!th]
%	\centering
%	\includegraphics[width=0.8\textwidth]{Figs/sci03_tld_framework.pdf}	
%	\caption{TLD Framework}
%	\label{fig:sci03_tld_framework}
%\end{figure*}
%
%
%In ensemble classifier, we compare the intensity values of several pixels to describe the target feature. Pair pixels comparison is one handy method to extract the target feature. Each bit of the feature code is calculated by
%\begin{align}
%f_i=\left\{ \begin{array}{ll}
%0 &\mbox{if $I(\mathbf{p}_i) < I(\mathbf{q}_i)$} \\
%1 &\mbox{otherwise} \end{array} \right.
%\end{align}
%where $I(\mathbf{p}_i)$ and $I(\mathbf{q}_i)$ is the gray scale of pixel at position $\mathbf{p}_i$  and $\mathbf{p}_i$ in image plane. An example of five-pair feature calculation for a single ferns shows in Figure \ref{fig:01_TLD_Code}. The random pairs method selects the five different location randomly and the result of each of these comparisons is interpreted as a binary digit, 10011. When written in decimal form, we get $F=19$. A probability could be obtained combing the other training data set.
%
%However, the random pairs is suitable for the target which could fully filled the subwindow, such as in face or pedestrian detection scenes. This kind of randomness could retrieve the feature of the target and adapts the variance during the tracking process. For fixed-wing MAV, this type of target only displays in the narrow part of the subwindow as shown in light yellow rectangle. Especially the wings of the MAV seems to be a single line when it is far away from the runway. The general randomness lose the kernel feature of the target occasionally, so we set one pixel of each pair randomly only in the yellow rectangle and the other pixel located in the rest part of the subwindow.  We named this process Semi-random Coding (SRC). 
%
%\begin{figure}[!th]
%	\centering
%	\includegraphics[width=0.8\textwidth]{Figs/01_TLD_Code.pdf}
%	\caption{We compare the intensities of different pairs for each image to get the binary code, then store it as decimal. Random pairs are generated randomly through the whole image plane. For semi-random pairs, one of the point is located randomly in the light yellow field and the other point is generated randomly at the rest part.}
%	\label{fig:01_TLD_Code}    
%\end{figure}
%\subsection{Bounding Box Shrinking (BBS) Algorithm}
%In order to enhance the localization accuracy, we need to detect the center of aircraft more precisely. However, the target center from the SRC-TLD result is the centroid of the bounding box which is not always true when the UAV was turning during the landing process. The left image of Figure \ref{fig:chp04_07_active_contour_demo} shows the standard results of SRC-TLD results. The green bouding box is the ground truth, and the red one is achieved from the SRC-TLD method. We took the bounding box shrinking (BBS) method to converge the predefined rectangle (the red rectangle) to the UAV main body as shown in the right image of Figure \ref{fig:chp04_07_active_contour_demo}.
%
%\begin{figure}[!th]
%	\centering
%	\includegraphics[width=0.8\textwidth]{Figs/chp04_07_active_contour_demo.pdf}
%	\caption{Bounding Box Shrinking Algorithm}
%	\label{fig:chp04_07_active_contour_demo}    
%\end{figure}
%
%Some previous works \cite{betser2004automatic} and \cite{sattigeri2007vision}, demonstrated that active contours are autonomous processes which employ image coherence in order to track various features of interest over time. Level set method is \cite{368173}, \cite{Caselles1993} in which an active contour model founded on the level set formulation of the Euclidean curve shortening equation is proposed. We tested the level set method as discussed in \cite{kong2013autonomous} and found that this method has limited processing frame because of the partial differential calculation. In 2014, the morphological-based approach was designed to calculate the curve evolution \cite{Marquez-Neila2014} which balances the operation speed and accuracy. 
%
%As derived in \cite{Marquez-Neila2014}, the level set implementation is 
%\begin{align}
%\frac{\partial u}{\partial t} = g(I)|\nabla u|\nu +g(I) |\nabla u|\text{div}(\frac{\nabla u}{|\nabla u|}) + \nabla g(I) \nabla u
%\end{align}
%where $g(I)$ is the function to select the interested part of the image; $u$ is the level set function and $\nu$ is the parameters. This equation involved three components: The first item is \textit{Balloon Force}, which makes the curve has the limited shrink velocity; the second item is \textit{Image Attraction Force} that is the feature-based principal contract force; \textit{Smoothing Force} flats segments where has high curvature, and this force is denoted as the third item.
%
%Instead of solving PDE function dirrectly, we could adopt morphological approach to calculate the above mentioned force more convenient and fast. These three contraction force can be established by morphological operator:
%\begin{align}
%\label{eq:PDE_1_3}
%u^{n+\frac{1}{3}}(\mathbf{x})=\left\{ \begin{array}{ll}
%(D_du^n)(\mathbf{x}) &\mbox{ if $g(I)(\mathbf{x})>\theta$ and $\nu>0_{ac}$} \\
%(E_du^n)(\mathbf{x}) &\mbox{ if $g(I)(\mathbf{x})>\theta$ and $\nu<0_{ac}$} \\
%u^{n}(\mathbf{x}) &\mbox{ otherwise}
%\end{array} \right.
%\end{align}
%
%\begin{align}
%\label{eq:PDE_2_3}
%u^{n+\frac{2}{3}}(\mathbf{x})=\left\{ \begin{array}{ll}
%1 &\mbox{ if $\nabla u^{n+\frac{1}{3}}(\mathbf{x}) \cdot \nabla g(I)(\mathbf{x}) > 0$} \\
%0&\mbox{ if $\nabla u^{n+\frac{1}{3}}(\mathbf{x}) \cdot \nabla g(I)(\mathbf{x}) < 0$} \\
%u^{n+\frac{1}{3}}(\mathbf{x}) &\mbox{otherwise}
%\end{array} \right.
%\end{align}
%
%\begin{align}
%\label{eq:PDE_3_3}
%u^{n+1} (\mathbf{x}) = (((SI_h) \circ (IS_h)) u^{n+\frac{2}{3}})(\mathbf{x})
%\end{align}
%where the $(D_du^n)(\mathbf{x})$ and $(E_du^n)(\mathbf{x})$ are dilation and erosion operator; $n$ is the curve evolution step. As the Eq.\ref{eq:PDE_1_3}, Eq.\ref{eq:PDE_2_3} and Eq.\ref{eq:PDE_3_3} illustrated, we divided the PDE calculation into three separated morphological steps. The SRC-TLD and BBS-based algorithm framework are shown in Alg.\ref{alg:active_contour}.
%
%\begin{algorithm2e}[t]
%	\SetAlgoLined
%	%	\KwData{this text}
%	%	\KwResult{how to write algorithm with \LaTeX2e }
%	\BlankLine
%	\SetKwInOut{Input}{Input}
%	\SetKwFunction{TLD}{TLD}
%	\SetKwFunction{SRC}{SRC}
%	\SetKwFunction{Update}{Update}
%	\SetKwFunction{Initialization}{Initialization}
%	\SetKwFunction{ContourInitialization}{ContourInitialization}
%	\SetKwFunction{BallForce}{BallForce}
%	\SetKwFunction{ImageAttractionForce}{ImageAttractionForce}
%	\SetKwFunction{SmoothingForce}{SmoothingForce}
%	\SetKwFunction{Length}{Length}
%	\SetKwFunction{TargetCenter}{TargetCenter}
%	\Input{Image sequence $I_1, ..., I_T$ with bounding box $b_1$}
%	\Initialization($I_1$, $b_1$)\;	
%	\For{$i=2$ \KwTo $T$}{
%		$b_i\ \leftarrow $ \SRC-\TLD{$I_i$}\;
%		$u^{1}\ \leftarrow$  \ContourInitialization($b_i$)\;
%		// or $u^{1}$ = \ContourInitialization($lastcountour$)\;
%		\For{$j=2$ \KwTo $Iterator$}{
%			$u^{j+\frac{1}{3}}\ \leftarrow$ \BallForce($u^{j-1}$)\;
%			$u^{j+\frac{2}{3}}\ \leftarrow$ \ImageAttractionForce($u^{j+\frac{1}{3}}$)\;
%			$u^{j}\ \leftarrow$ \SmoothingForce($u^{j+\frac{2}{3}}$)\;
%			\If { \Length($u^{j}$)-\Length($u^{j-1} < 5$) or \Length($u^{j}< 20$)}{
%				break \;			 
%			}			
%		}	
%		$lastcountour\ \leftarrow$ \TargetCenter($u^{j}$)\;     
%	}
%	\caption{SRC-TLD and BBS Algorithm}
%	\label{alg:active_contour}
%\end{algorithm2e}

\subsection{Bounding Box Prediction (BBP)}
In most tracking scenarios, the camera is fixed and algorithms could calculate the target position in next frame based on the current one and its movement pattern. Unlike with traditional method for ROI region prediction, the PTU rotation status should be considered to improve the prediction performance. Figure \ref{fig:chp04_02_pan_tilt_status} shows the pan and tilt rotation status of left and right guidance module, which recorded by the log system during a landing process. The red line in each diagram shows the delta angle between each time stamp and it is obvious that the PTU rotates not in a stable speed.
\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/chp04_02_pan_tilt_status.pdf}	
	\caption{Scale Filter Progress}
	\label{fig:chp04_02_pan_tilt_status}
\end{figure*}



\begin{Theorem}
The target center position can be predicted iteratively by
\begin{align}
\label{eq:prev_predict_curr}
x_{t} = f\frac{x_{t-1} - f\psi }{x_{t-1}  \psi  - \phi y_{t-1} + f} \\
y_{t} = f\frac{y_{t-1} -f\phi}{  x_{t-1} \psi - \phi y_{t-1} + f}
\end{align}
where $\psi$ and $\phi$ are the PTU rotation angles and the $f$ is the camera focal length.
\end{Theorem}
\begin{proof}[Proof of Theorem 2]

\begin{equation}
P_r(t) = T_c(t) P_c(t)
\end{equation}
where $P_c(t)$ and $P_r(t)$ are the target position in camera and guidance coordinate respectively. $T_c(t)$ is a $4 \times 4$ transformation matrix. 
Considering the rotation of PTU only in yaw and roll axis, the matrix is then given by
\begin{equation}
T_c(t) = R_Y(\psi)R_X(\phi)T_c(t-1)
\end{equation}
If the target is fixed during the ajacent time stamp, the position of the target can be presented by
\begin{equation}
P_r = T_c(t) P_c(t) =T_c(t-1) P_c(t-1)
\end{equation}
Because the minimum PTU rotation angle is $0.006 \degree$, the approximation of the angles are
\begin{align}
&\cos \phi \approx \cos \psi \approx 1 \\
&\sin \phi \approx \phi \\
&\cos \psi \approx \psi \\
&\sin \phi \approx \sin \psi \approx 0
\end{align}
If there is a initial rotation bias $\theta$ in horizontal axis, the transformation matrix is 
\begin{equation}
T_c(t) =\begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & \cos \theta & -\sin \theta & \rho_y \\0 & \sin \theta & \cos \theta & \rho_z \\ 0 & 0 & 0 &0 \end{pmatrix}
\end{equation}
Then the position of the target is
\begin{align}
&X_{t-1} = X_t + \psi \sin \theta Y_t + \psi \cos \theta Z_t + \psi \rho_z \\
&Y_{t-1} = -\psi \sin \theta X_t +  Y_ t - \phi Z_t + \phi \sin \theta \rho_y - \phi \cos \theta \rho_z \\
&Z_{t-1}  = -\psi \cos \theta X_t + \phi  Y_ t + Z_t + \phi \cos \theta \rho_y - \phi \sin \theta \rho_z
\end{align}
Using the traditional pin-hole camera model
\begin{equation}
x = f \frac{X}{Z}
\end{equation}
and ignoring the initial bias, we obtain the predicted target center position by
\begin{align}
\label{eq:prev_predict_curr}
x_{t} = f\frac{x_{t-1} - f\psi }{x_{t-1}  \psi  - \phi y_{t-1} + f} \\
y_{t} = f\frac{y_{t-1} -f\phi}{  x_{t-1} \psi - \phi y_{t-1} + f}
\end{align}
\end{proof}

Figure \ref{fig:chp04_17_predict_1} illustrates consecutive frames when the pitch angle jumped during the landing process. The figure on the left side is the $138\ th$ frame. Based on the moving pattern of UAV, without the PTU rotation status, we could get the predict center based on original method as shown in a yellow dot on the right side of the figure. The blue dot on the same figure shows the proposed prediction method result. It is obvious that the blue dot position is much near to the green one which is the ground truth target center.

\begin{figure}[!th]
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/chp04_17_predict_1.pdf}
	\caption{The horizontal line changed enormously caused by PTU pitch angel jump. }
	\label{fig:chp04_17_predict_1}    
\end{figure}


\section{Experiments and Discussion}
\subsection{Experiments Setup}

For visible light camera, we selected DFK 23G445 which developed by Imaging Source GmbH shown in Figure  \ref{fig:CameraOnly}. The sensor of this camera is Sony ICX445AQA equipped with GigE interface which has high data transfer rates, typically up to $1000\ Mbit/s$. This camera has an image resolution of 1280$\times$960 with RGB32 color model, a maximum frame rate of $30\ fps$. The lens of the vision system we adopted is $100\ mm$, and the baseline is $10\ m$. 
%The ground station, which runs the tracking and localization algorithms, is mainly constituted by ADLINK EOS-1200 PC, which sends relative position and records GNSS data from UAV as ground truth.  This product is a rugged and compact system equipped with Intel Core i7 2.1 GHz processor and 8 GB DDR3. 


\begin{figure}[!tb]
	\centering
	\subfigure[]
	{
		\includegraphics[height=1.6cm]{Figs/CameraOnly.pdf}
		\label{fig:CameraOnly}
	}
	\subfigure[]
	{
		\includegraphics[height=2.6cm]{Figs/PTU_D300E.pdf}
		\label{fig:PTU_D300E}
	}
	\caption{(a) DFK 23G445 Camera (b) PTU-D300E}
\end{figure}
To extend the field of view, we adopted precision PTU to actuate the camera. PTU-D300E (see Figure  \ref{fig:PTU_D300E} is a high performance product from FLIR. Its pan/tilt speeds up to $50\degree/second$ with the position resolution of $0.006\degree$. Moreover, it is a user programmable product integrating Ethernet  and RS-232 interface. The real-time command interface supports advanced applications such as video tracking. We set up the camera on the top bracketing, and the assembled individual vision system is illustrated in Figure  \ref{fig:SystemStructure}. 

This experimental test-bed is  a customized fixed wing aircraft as shown in Figure  \ref{fig:Kaitudozhe_VIGA}, which is a gasoline-powered radio-controlled model aircraft. The on-board autopilot allowed for the aircraft to perform simple commanded maneuver. Our autopilot module is iFLY-F1A (Figure  \ref{fig:iFly_F1A} and the navigation module is iFLY-G2 (Figure  \ref{fig:iFly_G2} \cite{IFLY}, which is a small six-DOF (degree of freedom) navigation system. This module supports real-time 3D information including attitude angle, angular rate, position, speed, acceleration, true air speed, calibrated air speed. F1A is connected with G2 through RS-232 serial port. Table \ref{tab:platform_specifications} lists the other technical specifications of the UAV platform.

\begin{figure}[!tb]
	\centering
	\subfigure[]
	{
		\includegraphics[height=3cm]{Figs/Kaituozhe_Our.pdf}
		\label{fig:Kaitudozhe_VIGA}
	}	
	\subfigure[]
	{
		\label{fig:iFly_F1A}
		\includegraphics[height=3cm]{Figs/iFly_F1A.pdf}
	}
	\subfigure[]
	{
		\label{fig:iFly_G2}
		\includegraphics[height=3cm]{Figs/iFly_G2.pdf}
	}	
	\caption{(a) The Middle-sized Fixed-wing Platform (b) iFLY-F1A Module   (c) iFLY-G2 Module}
\end{figure}



\begin{table}
	\caption{The Technical Specifications of Pioneer}
	\label{tab:platform_specifications}
	\begin{center}
		\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{lll}
			\hline
			\textbf {Items}  & \textbf{Description} \\
			\hline
			Vehicle mass & 9000 g \\
			Maximum Payload mass & 5000 g \\
			Diameter & 2900 mm \\
			Flight duration & up to 180 minutes \\
			Cruising speed & 30.0 m/s \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

Communication is curcial in the landing guidance framework, because the relative localization is broadcast through the radio. The guidance data are sent using an advanced radio modem that transmits and receives on the $900\ Mhz$ band. The XTend RF Modems support up to $22\ km$ outdoor communication with the interface data rates from $10\ bps$ to $230,00\ bps$, which is sufficient to transfer GNSS data and predicted position from ground station to on-board navigation modem. 


\subsection{Tracking Algorithm Experiments}
To evaluate the tracking accuracy, we run the proposed algorithm with seven landing datasets. There are four landing sequence recorded by a small-sized UAV and the others are by a middle-sized one. We set the $\lambda=1$ and $\mu=0.025$. Figure \ref{fig:chp04_11_small_uav_cle}, Figure \ref{fig:chp04_12_small_uav_overlap}, Figure \ref{fig:chp04_13_middle_uav_cle} and Figure \ref{fig:chp04_14_middle_uav_overlap} illustrate center location error and bounding box overlap percentage result with different UAV platform. For small-sized UAV, there was a significant tracking disturbance betwen 200 to 250 frames due to the unstable PTU control and the limited features of the target. The algorithm operates well in both indices middle-sized platform. However, due to the larger ROI region, the algorithm cost more time during the DFT calculation. Tabel \ref{tab:DSST_Tracking} shows the seven racking results in average. 

\begin{figure}[!tb]
	\centering
	\subfigure[]
	{
		\includegraphics[height=3.3cm]{Figs/chp04_11_small_uav_cle.pdf}
		\label{fig:chp04_11_small_uav_cle}
	}
	\subfigure[]
	{
		\includegraphics[height=3.3cm]{Figs/chp04_12_small_uav_overlap.pdf}
		\label{fig:chp04_12_small_uav_overlap}
	}
	\caption{DSST Experiments in Small-sized UAV Sequence 1 (a) Center Location Error in X- and Y-axis  (b) Bounding Box Overlap Percentage}
\end{figure}

\begin{figure}[!tb]
	\centering
	\subfigure[]
	{
		\includegraphics[height=3.3cm]{Figs/chp04_13_middle_uav_cle.pdf}
		\label{fig:chp04_13_middle_uav_cle}
	}
	\subfigure[]
	{
		\includegraphics[height=3.3cm]{Figs/chp04_14_middle_uav_overlap.pdf}
		\label{fig:chp04_14_middle_uav_overlap}
	}
	\caption{DSST Experiments in Middle-sized UAV Sequence 1 (a) Center Location Error in X- and Y-axis  (b) Bounding Box Overlap Percentage}
\end{figure}



\begin{table}[]
	\centering
	\caption{DSST Tracking Algorithm Experiments}
	\label{tab:DSST_Tracking}
	\begin{tabular}{cccccc}
		\hline
		\multicolumn{1}{l}{\textbf{}} & \multicolumn{1}{l}{\textbf{Sequence}} & \textbf{\begin{tabular}[c]{@{}c@{}}Center Location \\ Error(Pixel)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Distance \\ Precision (\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Overlap\\ Precision(\%)\end{tabular}} & \textbf{Speed (fps)} \\ \hline
		\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Small-sized\\ UAV\end{tabular}}} & 1 & 3.41 & 97.53 & 74.32 & 32.31 \\
		& 2 & 3.27 & 98.12 & 75.23 & 30.13 \\
		& 3 & 3.12 & 98.23 & 74.34 & 29.12 \\
		& 4 & 2.80 & 96.84 & 74.32 & 31.84 \\ \hline
		\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Middle-sized\\ UAV\end{tabular}}} & 1 & 4.67 & 94.94 & 85.35 & 10.65 \\
		& 2 & 4.93 & 95.14 & 83.35 & 12.31 \\
		& 3 & 4.12 & 95.31 & 82.66 & 11.98 \\ \hline
	\end{tabular}
\end{table}

Obviously, the proposed method can track the target center correctly and the speed for the small-sized UAV tracking is around $30 fps$.
%To compare with the standard TLD method, the stability of Semi-random Code method was estimated for each frame by means of running 1000 Monte-Carlo experiments in one landing image sequence. Table \ref{lab:TLD_params} lists the parameters for TLD and SRC-TLD and the result is shown as in Figure \ref{fig:chp04_24_random_semi_random_monte_carlo}. The line shows the center location error for each algorithm, and the continuous shaded area is the error region, which explicate that the SRC has better stability and the accuracy of center location is improved. 

%\begin{table}[!th]
%	\centering
%	\caption{Parameters for TLD and SRC-TLD}
%	\label{lab:TLD_params}
%	\begin{tabular}{cc}
%		\hline
%		& \textbf{Parameters} \\ \hline
%		\textbf{Min/Max Scale} & 10 \\
%		\textbf{Features Number} & 8 \\
%		\textbf{Trees Number} & 15 \\
%		\textbf{Learning Ratio} & 0.8 \\ \hline
%	\end{tabular}
%\end{table}

%\begin{figure}[!th]
%	\centering
%	\includegraphics[width=0.8\textwidth]{Figs/chp04_24_random_semi_random_monte_carlo.pdf}
%	\caption{Comparision between random and semi-random coding}
%	\label{fig:chp04_24_random_semi_random_monte_carlo}    
%\end{figure}

%\begin{figure}[!th]
%	\centering
%	\includegraphics[width=0.6\textwidth]{Figs/chp04_05_landing_data_1_left_Contour.pdf}
%	\caption{All tracking results in sequence 1 with SRC-TLD and BBS}
%	\label{fig:chp04_05_landing_data_1_left_Contour}    
%\end{figure}

%Also, we compared SRC-TLD with Meanshift, AdaBoost and standard TLD methods under different size of UAVs in seven image sequences. The results was shown in Table \ref{lab:TLD_with_others}. The center location error (CLE) of our methods are all within $7\ pixel$ accuracy, which is the best compare with the other real-time tracking algorithms and the fatest process frame rate is $21.22 fps$. SRC-TLD method enhance the CLE around $2\ pixel$ both in small-sized UAV and middle-sized UAV sequences.

\begin{table*}[ht]
	\centering
	\caption{Target Detection Precision in Image Plane}
	\label{lab:TLD_with_others} 
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{ccccccc}
			\hline
			\multicolumn{1}{l}{\multirow{2}{*}{}}                               & \multirow{2}{*}{\textbf{Sequence}} & \multirow{2}{*}{\textbf{Frames}} & \textbf{AdaBoost}     & \textbf{Meanshift}     & \textbf{TLD}          & \begin{tabular}[c]{@{}c@{}} \textbf{DDST with} \\ \textbf{BBP} \end{tabular} \\ \cline{4-7} 
			\multicolumn{1}{l}{}                                                &                           &                         & \multicolumn{4}{c}{Center Location Error in Image Plane (pixel) / Speed(fps)}                                                     \\ \hline
			\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Small-sized}\\ \textbf{UAV}\end{tabular}} & 1                         & 355                     & 6.31 / 9.51  & 14.39 / 15.35 & {6.14} / 21.13 & 3.41 / {32.31}                                                         \\
			& 2                         & 384                     & 7.47 / 10.81 & 14.25 / 15.11 & 7.37 / 20.91 & {3.27} / {30.13}                                                         \\
			& 3                         & 340                     & 6.61 / 10.14 & 13.48 / 15.33 & 7.62 / {21.10} & {3.12} / 29.12                                                         \\
			& 4                         & 338                     & 7.77 / 10.12 & 13.85 / 14.71 & 7.72 / {20.92} & {2.80} / 31.84                                                         \\ \hline
			\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Middle-sized}\\ \textbf{UAV} \end{tabular}} & 1                         & 137                     & 8.01 / 8.33  & 11.41 / 13.33 & 8.85 / {17.00} & {4.67} / 10.65                                                         \\
			& 2                         & 159                     & 9.65 / 8.59  & 13.10 / 13.67 & 8.95 / 16.96 & {4.93} / {12.31}                                                         \\
			& 3                         & 181                     & 8.33 / 8.01  & 11.13 / 13.03 & 8.93 / 17.37 & {4.12} / {11.98}                                                         \\ \hline
		\end{tabular}% 
	}
\end{table*}

\subsection{Field Experiments}
\subsubsection{Scenarios}

The landing procedure were divided into four sections: (1) The UAV takeoff from the runway. (2) Cruise near the landing area in a large range to test the control system. (3) Cruise near the landing area in a small range and after the UAV is locked by visual system and received the visual references, the UAV control system was using vision-based localization data and the GPS data was only recorded as the benchmark. (4) Safely landing back to the runway.

In the basic setup, we separate the vehicle guidance and control into an inner loop and an outer loop, because it is much simpler and well-tested design approach. As the inner loop controller is already exist in the autopilot, we developed an efficient and robust outer guidance loop, which manage the visual information with the on-board sensors.

%%% Selected Sentences



\subsubsection{Real-flight Experiments}
Based on the results of simulation, eight sets of experimental results are conduced to establish the feasibility of the proposed approach. In realistic application, it is very critical requirements that the lateral deviation error from the middle line of the runway and the lateral acceleration of the vehicle should be perfectly eliminated to minimize the damage of the vehicle. Figure \ref{fig:sci02_landing_results_big} illustrate the approaching resutls. The left image shows the landing waypoints projecting on a satellite map where $A$ is the locking point of the ground landing and $B$ is the desired touch down point on the runway. In addition, the three 3D landing curves represent the calculated results from TLD, DDST and DDST with BBP methods. To compare with the ground truth, recording during the landing process by DGPS, the location errors of each axis are lists on the right side. In X and Z-axis, the location error decrease while the vehicle approach the landing area. The error in Y-axis has larger error comparing with X and Z-axis and the disturbance is significant.

\begin{figure*}[!th]
	\centering
	\includegraphics[width=\textwidth]{Figs/sci02_landing_results_big.pdf}	
	\caption{Fixed-wing Landing Final Approaching}
	\label{fig:sci02_landing_results_big}
\end{figure*}


\begin{table*}[!th]
	\centering
	\caption{Eight Experiment Results in Different Weather Condition}
	\label{lab:eight_ground_landing}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{cccccc}
			\hline
			\multicolumn{1}{l}{\textbf{No.}} & \multicolumn{1}{l}{\textbf{Weather Condition}} & \multicolumn{1}{l}{\textbf{Detection Distance}} & \multicolumn{1}{l}{\textbf{RMSE $\mathbf{i}^{O,c}$(m)}} & \multicolumn{1}{l}{\textbf{RMSE $\mathbf{j}^{O,c}$(m)}} & \multicolumn{1}{l}{\textbf{RMSE $\mathbf{k}^{O,c}$(m)}} \\ \hline
			\textbf{1} & Clear & 848.735 & \underline{0.330} & \underline{1.465} & 0.291 \\
			\textbf{2} & Clear & \textbf{892.134} & 0.241 & 1.292 & \textbf{0.229} \\
			\textbf{3} & Clear & 872.311 & 0.389 & 1.322  & 0.293 \\
			\textbf{4} & Clear & \underline{847.373} & 0.261 & 1.413 & 0.245 \\
			\textbf{5} & Clear & 857.117 & \textbf{0.251} & \textbf{1.252} & \underline{0.312} \\ \hline
			\textbf{6} & Overcast & \underline{491.193} & \underline{0.503} & 1.689 & \underline{0.602} \\
			\textbf{7} & Overcast & 503.175 & 0.495 & \textbf{1.353} & \textbf{0.587} \\
			\textbf{8} & Overcast & \textbf{534.238} & \textbf{0.482} & \underline{1.781} & 0.592 \\ \hline
		\end{tabular}
	}		
\end{table*}

As the theoretical and simulation result discussed, the localization errors in each axis are large when the UAV is far way from the ground visual system. To illustrate the result more clearly, we compared the localization results with DGPS at separated intervals which is shown in Tabel. \ref{lab:ground_landing}. Previously, the average error of each axis at large distance (more than $400\ m$) are large, especially in depth dimension. The comparison of 3D localization error between various lists in Table  \ref{lab:3D_Error_Algorithms}. The SRC-TLD results has the best real-time performance which reaches $21.345\ fps$ and has better accuracy compare with Meanshift method which has similar process speed. In accuracy measurement, the SRC-TLD with BBS calculate the 3D position more precisely at the cost of slower frame rate.

\begin{table*}[!th]
	\centering
	\caption{Guidance Error in Each Axes at Separated Interval with DSST and BBP Algorithms}
	\label{lab:ground_landing}
	\begin{tabular}{cccc|cccc}
		\hline
		\textbf{Interval(m)} & \textbf{$\mathbf{i}^{O,c}$(m)} & \textbf{$\mathbf{j}^{O,c}$(m)} & \textbf{$\mathbf{k}^{O,c}$(m)} & \textbf{Interval(m)} & \textbf{$\mathbf{i}^{O,c}$(m)} & \textbf{$\mathbf{j}^{O,c}$(m)} & \textbf{$\mathbf{k}^{O,c}$(m)} \\ \hline
		\textbf{600$\sim$580} & 0.348 & 1.862 & 0.441 & \textbf{300$\sim$280} & 0.138 & 1.526 & 0.153\\
		\textbf{580$\sim$560} & 0.322 & 1.444 & 0.362 & \textbf{280$\sim$260} & 0.142 & 1.413 & 0.178 \\
		\textbf{560$\sim$540} & 0.218 & 1.557 & 0.327 & \textbf{260$\sim$240} & 0.103 & 1.114 & 0.124 \\
		\textbf{540$\sim$520} & 0.197 & 1.558 & 0.284 & \textbf{240$\sim$220} & 0.094 & 0.711 & 0.132 \\
		\textbf{520$\sim$500} & 0.228 & 1.841 & 0.183 & \textbf{220$\sim$200} & 0.105 & 0.898 & 0.143 \\
		\textbf{500$\sim$480} & 0.229 & 1.430 & 0.226 & \textbf{200$\sim$180} & 0.151 & 0.831 & 0.163 \\
		\textbf{480$\sim$460} & 0.192 & 1.483 & 0.233 & \textbf{180$\sim$160} & 0.163 & 0.842 & 0.134 \\
		\textbf{460$\sim$440} & 0.183 & 1.472 & 0.239 & \textbf{160$\sim$140} & 0.157 & 0.913 & 0.192 \\
		\textbf{440$\sim$420} & 0.192 & 1.431 & 0.121 & \textbf{140$\sim$120} & 0.142 & 0.725 & 0.160 \\
		\textbf{420$\sim$400} & 0.191 & 1.663 & 0.199 & \textbf{120$\sim$100} & 0.169 & 0.922 & 0.149 \\
		\textbf{400$\sim$380} & 0.169 & 1.662 & 0.193 & \textbf{100$\sim$80} & 0.147 & 0.797 & 0.069 \\
		\textbf{380$\sim$360} & 0.171 & 1.542 & 0.185 & \textbf{80$\sim$60} & 0.133 & 0.697 & 0.079 \\
		\textbf{360$\sim$340} & 0.173 & 1.541 & 0.183 & \textbf{60$\sim$40} & 0.114 & 0.441 & 0.068 \\
		\textbf{340$\sim$320} & 0.153 & 1.333 & 0.161 & \textbf{40$\sim$20} & 0.124 & 0.312 & 0.064 \\
		\textbf{320$\sim$300} & 0.156 & 1.311 & 0.163 & \textbf{20$\sim$00} & 0.082 & 0.284 & 0.103 \\ \hline
	\end{tabular}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!th]
	\centering
	\caption{UAV 3D Localization Error with Tracking Algorithms}
	\label{lab:3D_Error_Algorithms}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{ccccc|cc}
			\hline
			& \multicolumn{4}{c|}{\textbf{Previous Methods}} & \multicolumn{2}{c}{\textbf{Our Methods}} \\ \cline{2-7} 
			\multicolumn{1}{l}{} & \textbf{Meanshift} & \textbf{AdaBoost} & \textbf{Chan-Vase} & \textbf{Saliency-inspired} & \textbf{DSST} & \textbf{DSST + BBP} \\ \hline
			\textbf{RMSE }$\mathbf{i}^{O,c}$\textbf{(m)} & 0.578 & 0.457 & 0.271 & 0.770 & 0.681 & \textbf{0.411} \\
			\textbf{RMSE }$\mathbf{j}^{O,c}$\textbf{(m)} & 5.241 & 2.421 & 1.811 & 1.763 & 1.601 & \textbf{1.394} \\
			\textbf{RMSE }$\mathbf{k}^{O,c}$\textbf{(m)} & 0.823 & 0.547 & 0.314 & 0.618 & 0.462 & \textbf{0.341} \\
			\hline
			\textbf{\begin{tabular}[c]{@{}c@{}}Average \\ Frame Rate (fps)\end{tabular}} & 20.131 & 13.152 & 7.131 & 8.013 & \textbf{28.335} & 27.867 \\ \hline
		\end{tabular}%
	}
\end{table}


\subsection{Conclusion}
This paper presents a complete framework of the ground-based stereo localization problem. This guidance system could be used to pilot the UAV landing autonomously and safely in GNSS-denied scenario. The DDST algorithm could detect the target faster than our previous works. Revising the bounding box by the BBP algorithm, the 3D localization error improved in average. Compared with the onboard solutions, this ground-based system profited enormously from the computation capacity and flexible configuration with base-line and sensors. Although the system has some pitfalls, such as the low accuracy at long distance in depth axis and not supporting the attitude measurement, this low-cost system could arrange quickly at any proposed environments. Additional future work will focus on estimate errors over time and investigate methods to improve inevitable error propagation through the inclusion of additional sensors, such as GNSS and on-board sensors.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Results}
%
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Subsection}
%
%\subsubsection{Subsubsection}
%
%Bulleted lists look like this:
%\begin{itemize}[leftmargin=*,labelsep=4mm]
%\item	First bullet
%\item	Second bullet
%\item	Third bullet
%\end{itemize}
%
%Numbered lists can be added as follows:
%\begin{enumerate}[leftmargin=*,labelsep=3mm]
%\item	First item
%\item	Second item
%\item	Third item
%\end{enumerate}
%
%The text continues here.
%
%\subsection{Figures, Tables and Schemes}
%
%All figures and tables should be cited in the main text as Figure 1, Table 1, etc.
%
%\begin{figure}[H]
%\centering
%\includegraphics[width=3cm]{logo-mdpi}
%\caption{This is a figure, Schemes follow the same formatting. If there are multiple panels, they should be listed as: (\textbf{a}) Description of what is contained in the first panel. (\textbf{b}) Description of what is contained in the second panel. Figures should be placed in the main text near to the first time they are cited. A caption on a single line should be centered.}
%\end{figure}   
%
%\begin{table}[H]
%\caption{This is a table caption. Tables should be placed in the main text near to the first time they are cited.}
%\centering
%%% \tablesize{} %% You can specify the fontsize here, e.g.  \tablesize{\footnotesize}. If commented out \small will be used.
%\begin{tabular}{ccc}
%\toprule
%\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
%\midrule
%entry 1		& data			& data\\
%entry 2		& data			& data\\
%\bottomrule
%\end{tabular}
%\end{table}
%
%\subsection{Formatting of Mathematical Components}
%
%This is an example of an equation:
%
%\begin{equation}
%\mathbb{S}
%\end{equation}
%
%%% If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph. 
%Please punctuate equations as regular text. Theorem-type environments (including propositions, lemmas, corollaries etc.) can be formatted as follows:
%%% Example of a theorem:
%\begin{Theorem}
%Example text of a theorem.
%\end{Theorem}
%
%The text continues here. Proofs must be formatted as follows:
%
%%% Example of a proof:
%\begin{proof}[Proof of Theorem 1]
%Text of the proof. Note that the phrase `of Theorem 1' is optional if it is clear which theorem is being referred to.
%\end{proof}
%The text continues here.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Discussion}
%
%Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Materials and Methods}
%
%Materials and Methods should be described with sufficient details to allow others to replicate and build on published results. Please note that publication of your manuscript implicates that you must make all materials, data, computer code, and protocols associated with the publication available to readers. Please disclose at the submission stage any restrictions on the availability of materials or information. New methods and protocols should be described in detail while well-established methods can be briefly described and appropriately cited.
%
%Research manuscripts reporting large datasets that are deposited in a publicly available database should specify where the data have been deposited and provide the relevant accession numbers. If the accession numbers have not yet been obtained at the time of submission, please state that they will be provided during review. They must be provided prior to publication.
%
%Interventionary studies involving animals or humans, and other studies require ethical approval must list the authority that provided approval and the corresponding ethical approval code. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusions}
%
%This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at www.mdpi.com/link, Figure S1: title, Table S1: title, Video S1: title.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\acknowledgments{All sources of funding of the study should be disclosed. Please clearly indicate grants that you have received in support of your research work. Clearly state if you received funds for covering the costs to publish in open access.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``X.X. and Y.Y. conceived and designed the experiments; X.X. performed the experiments; X.X. and Y.Y. analyzed the data; W.W. contributed reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be limited to those who have contributed substantially to the work reported.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funding sponsors in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript, or in the decision to publish the results must be declared in this section. If there is no role, please state ``The founding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, and in the decision to publish the results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\abbreviations{The following abbreviations are used in this manuscript:\\
%
%\noindent 
%\begin{tabular}{@{}ll}
%MDPI & Multidisciplinary Digital Publishing Institute\\
%DOAJ & Directory of open access journals\\
%TLA & Three letter acronym\\
%LD & linear dichroism
%\end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\appendixtitles{no} %Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
%\appendixsections{multiple} %Leave argument "multiple" if there are multiple sections. Then a counter is printed ("Appendix A"). If there is only one appendix section then change the argument to "one" and no counter is printed ("Appendix").
%\appendix
%\section{}
%\subsection{}
%The appendix is an optional section that can contain details and data supplemental to the main text. For example, explanations of experimental details that would disrupt the flow of the main text, but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data is shown in the main text can be added here if brief, or as Supplementary data. Mathematical proofs of results not central to the paper can be added as an appendix.
%
%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 
\bibliographystyle{mdpi}

%=====================================
% References, variant A: internal bibliography
%=====================================
%\renewcommand\bibname{References}
%\begin{thebibliography}{999}
%% Reference 1
%\bibitem{ref-journal}
%Lastname, F.; Author, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142-149.
%% Reference 2
%\bibitem{ref-book}
%Lastname, F.F.; Author, T. The title of the cited contribution. In {\em The Book Title}; Editor, F., Meditor, A., Eds.; Publishing House: City, Country, 2007; pp. 32-58.
%\end{thebibliography}

%=====================================
% References, variant B: external bibliography
%=====================================
%\bibliographystyle{IEEEtran}
\bibliography{template}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\sampleavailability{Samples of the compounds ...... are available from the authors.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

